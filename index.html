<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>TDM</title>
<link href="./yoso_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./yoso_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./yoso_files/jquery.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<style>
  p.serif{
    font-family:"Times New Roman", Times, serif;
  }
  p.sansserif{
    font-family: Arial, Helvetica, sans-serif;
  }
</style>
  
</head>

<body>
<div class="content">
  <h1><strong>Learning Few-Step Diffusion Models by Trajectory Distribution Matching
  </strong>&nbsp;&nbsp;</h1>
  <p id="authors" class="serif">
    <a href="https://scholar.google.com/citations?user=9VfuwdsAAAAJ&hl=zh-CN&oi=ao">Yihong Luo<sup>1<dag></sup></a>
    <a href="https://hu-tianyang.github.io">Tianyang Hu<sup>2</sup></a>
    <a href="https://openreview.net/profile?id=~Jiacheng_Sun1">Jiacheng Sun<sup>2</sup></a>
    <a href="https://vanoracai.github.io/">Yujun Cai<sup>3</sup></a> 
    <a href="https://sites.google.com/view/jtang">Jing Tang<sup>4,1✝<dag></sup></a>
    <br>
    <a style="font-size: 0.7em"><sup>✝</sup>Corresponding Author.</a>
    <br>
    <span style="font-size: 0.8em; margin-top: 0.5em">
      <a><sup>1</sup>The Hong Kong University of Science and Technology</a>
      <br>
      <a><sup>2</sup>HuaweiNoah's Ark Lab</a>
      <br>
      <a><sup>3</sup>National University of Singapore</a>
      <br>
      <a><sup>4</sup>The Hong Kong University of Science and Technology (Guangzhou)</a>
    </span>
  </p>

  <font size="+1">
    <p style="text-align: center;" class="serif">
      <!-- <a href="https://www.arxiv.org/abs/2403.12931" target="_blank"  style="font-weight: bold;">[Paper]</a>&nbsp;&nbsp;&nbsp;&nbsp; -->
      <a href="https://github.com/Luo-Yihong/TDM" target="_blank" style="font-weight: bold;">[Github]</a>&nbsp;&nbsp;&nbsp;&nbsp;
      <!-- <a href="#bibtex" style="font-weight: bold;">[BibTeX]</a> -->
    </p><br>
  </font>

  <div style="display: flex; justify-content: space-between;">
    <div style="width:49%; padding: 5px;">
      <video controls style="width:100%;">
        <source src="videos/teacher.mp4" type="video/mp4">
      </video>
      <p style="text-align: center;" class="serif"> (1) Tacher Samples (CogVideoX-2B 100 NFE). </p>
    </div>
    
    <div style="width:49%; padding: 5px;">
      <video controls style="width:100%;">
        <source src="videos/student.mp4" type="video/mp4">
      </video>
      <p style="text-align: center;" class="serif"> (2) <b>TDM Samples (4NFE)</b>. </p>
    </div>
  </div>
  <p style="font-size: 1.2em" class="serif">
  The Left is generated by CogVideoX-2B (100 NFE) on a single 4090 by around 90s. At the same time, our TDM (4 NFE) can generate 25 videos, as shown on the right! <b> 25 times faster without performance degradation!</b> 
  </p>

  <!-- <img class="summary-img" src="figs/yoso.png" style="width:100%;">  -->

</div>

<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Abstract</p>
  <p style="font-size: 1.2em" class="serif">
    Accelerating diffusion model sampling is crucial for efficient AIGC deployment. 
While diffusion distillation methods---based on distribution matching and trajectory matching --- reduce sampling to as few as one step, they fall short on complex tasks like text-to-image generation. 
Few-step generation offers a better balance between speed and quality, but existing approaches face a persistent trade-off: distribution matching lacks flexibility for multi-step sampling, while trajectory matching often yields suboptimal image quality.
To bridge this gap, we propose learning few-step diffusion models by Trajectory Distribution Matching (TDM), a novel framework that combines the strengths of distribution and trajectory matching. 
Our method introduces a data-free score distillation objective, aligning the student's trajectory with the teacher's at the distribution level. 
Further, we develop a sampling-steps-aware objective that decouples learning targets across different steps, enabling more adjustable sampling.
This approach supports both deterministic sampling for superior image quality and flexible multi-step adaptation, achieving state-of-the-art performance with remarkable efficiency. 
Our model, TDM, outperforms existing methods on various backbones, such as SDXL and PixArt-\(\alpha\), delivering superior quality and significantly reduced training costs.
In particular, our method distills PixArt-\(\alpha\) into a 4-step generator that outperforms its teacher on real user preference at 1024 resolution. This is accomplished with 500 iterations and 2 A800 hours---a mere 0.01% of the teacher's training cost.
  </p>
</div>

<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">User Study Time!</p>
  <p style="font-size: 1.2em" class="serif">
    <img class="summary-img" src="figs/user_study.jpg" style="width:100%;"> 
    Which one do you think is better? Some images are generated by Pixart-\(\alpha\) (50 NFE). Some images are generated by <b>TDM (4 NFE)</b>, distilling from Pixart-\(\alpha\) in a data-free way with merely 500 training iterations and 2 A800 hours. All images are generated from the same initial noise.  
  </p>
  <details style="margin-top: 15px;">
    <summary class="serif"style="color: #1E88E5; cursor: pointer; font-size: 1.2em; outline: none" >Click for answer</summary>
    <p style="font-size: 1.2em; margin-top: 8px;" class="serif">
      Answers of TDM's position (left to right): bottom, bottom, top, bottom, top.
    </p>
  </details>
</div>

<style>
/* 隐藏默认三角 */
details summary::-webkit-details-marker {
  display: none;
}

/* 自定义箭头动画 */
details summary::before {
  content: "▶";
  display: inline-block;
  margin-right: 8px;
  transition: transform 0.2s;
}

details[open] summary::before {
  transform: rotate(90deg);
}
</style>
</div>



<!-- <div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">User Study Time!</p>
  <p style="font-size: 1.2em" class="serif">
    <img class="summary-img" src="figs/user_study.jpg" style="width:100%;"> 
    Which one do you think is better? Some images are generated by Pixart-\(\alpha\) (50 NFE). Some images are generated by <b>TDM (4 NFE)</b>, distilling from Pixart-\(\alpha\) in a data-free way with merely 500 training iterations and 2 A800 hours. All images are generated from the same initial noise.  
  </p>
  <p style="font-size: 1.2em" class="serif">
    Answers of TDM's position: (left to right): bottom, bottom, top, bottom, top.
  </p>
  </div> -->

<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Methodology</p>
  <p style="font-size: 1.2em" class="serif">
    Our TDM builds non-trival connection between trajectory distillation and distribution matching, deilivering a new distillation paradigm: Trajectory Distribution Matching.     
    TDM combines the strengths of distribution and trajectory matching. Our method introduces a data-free score distillation objective, aligning the student's trajectory with the teacher's at the distribution level. 

    <br>

    Benefiting from the proposed design, our method possesses the following advantages: <br>
    <br>
    1) Supporting various ODE samplers for both training and inference, <br>

    2) A ultra fast training process, <br>

    3)  Extremely fast high-quality few-step generation: TDM can suparrs the teacher without extra high-quality data in real user preference!
  </p>
  
  <!-- <img class="summary-img" src="figs/framework-final-new-0523.pdf" style="width:100%;"> -->
  <img class="summary-img" src="figs/frame.jpg" style="width:100%;">
  
</div>

<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Qualitative Comparison</p>
  <p style="font-size: 1.2em" class="serif">
    We present the qualitative comparison to existing SOTA methods below.
    It is clear that our method has better visual quality and text-image alignment compared to competing baselines and even the teacher diffusion with 25 steps.
    <!-- We recommend users visit our github repo and play with our pre-trained weights. -->
  </p>
  <img class="summary-img" src="figs/visual_compare.jpg" style="width:100%;"> 
</div>


<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Real User Preference</p>
  <p style="font-size: 1.2em" class="serif">
    To further verify the effectiveness of our proposed method, we conduct an extensive user study across different backbones. 
    The results are shown in below: Our method clearly outperforms the teacher diffusion and other most competing methods in a image-free way.
  </p>
  <img class="summary-img" src="figs/user_preference.jpg" style="width:100%;"> 
</div>


<!-- </div> -->

<!-- <div class="content" id="bibtex">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">BibTeX</p>
  <code> @misc{luo2024sample,<br>
  &nbsp;&nbsp;title={You Only Sample Once: Taming One-Step Text-to-Image Synthesis by Self-Cooperative Diffusion GANs},<br>
  &nbsp;&nbsp;author={Yihong Luo and Xiaolong Chen and Xinghua Qu and Jing Tang},<br>
  &nbsp;&nbsp;year={2024},<br>
  &nbsp;&nbsp;eprint={2403.12931},<br>
  &nbsp;&nbsp;archivePrefix={arXiv},<br>
  &nbsp;&nbsp;primaryClass={cs.CV}<br>
  } </code> 
</div> -->

<div class="content">
  <p class="serif">
    Project page template is borrowed from <a href="https://dreambooth.github.io/">DreamBooth</a>.
  </p>
</div>

</body>

</html>
